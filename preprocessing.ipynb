{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76b2fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "PRE_TRAINED_MODEL = 'cimm-kzn/enrudr-bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aabedf6",
   "metadata": {},
   "source": [
    "Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d90e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {' NEUTRAL_EMOJI ': 'ğŸ¤¨ğŸ¤«ğŸ¤”ğŸ˜¶ğŸ™ˆğŸ™„ğŸ˜¬ğŸ¤·ğŸ˜‘ğŸ˜ğŸ˜µğŸ˜²ğŸ˜—ğŸ˜®ğŸ˜³ğŸ˜¯',\n",
    "             ' POS_EMOJI ': \"\"\"ğŸ‘ŒğŸ‘ğŸ˜‚ğŸ˜€ğŸ¤­ğŸ™ƒğŸ¥°â¤ï¸ğŸ˜˜ğŸ˜ğŸ–¤ğŸ˜â™¥ğŸ’œğŸ¤£ğŸ’•ğŸ˜…ğŸ’šğŸ’™ğŸ˜‰ğŸ˜†ğŸ˜ğŸ˜ŠğŸŒšğŸ˜ƒğŸ˜ğŸ§¡ğŸ˜‹ğŸ¤ªğŸ™‚â˜»ğŸ˜„ğŸ’–ğŸ¤¤â˜ºğŸ¤ ğŸ˜‡ğŸŒğŸ¤©ğŸ˜œ\\\n",
    "ğŸ˜ŒğŸ§ğŸ’—ğŸ¤—ğŸ‘©ğŸ˜¹ğŸ˜¸ğŸ’›ğŸ˜â™¡ğŸ‘ğŸ’“ğŸ˜›âœŒ\"\"\",\n",
    "             ' NEG_EMOJI ': 'ğŸ˜•ğŸ˜«ğŸ¤¬ğŸ¤¦ğŸ˜±ğŸ˜°ğŸ¥´ğŸ˜­ğŸ’”ğŸ˜”ğŸ˜¥ğŸ‘ğŸ¤’ğŸ˜©ğŸ¤•ğŸ˜¨ğŸ¥µğŸ˜¢ğŸ˜–ğŸ˜¡â˜¹ï¸ğŸ˜ªğŸ¥ºğŸ’€ğŸ˜“ğŸ˜£ğŸ˜ğŸ˜¿ğŸ˜ ğŸ˜ŸğŸ‘¿ğŸ˜’ğŸ˜¦',\n",
    "             ' HEALTH_EMOJI ': 'ğŸš‘ğŸ¤§ğŸ’ŠğŸ’‰ğŸ˜·ğŸ¥',\n",
    "             '!': 'Â¡â—ï¸',\n",
    "             '?': 'Â¿â“',\n",
    "             '!?': 'â‰',\n",
    "             '(': 'ï¼ˆ',\n",
    "             '.': 'ã€‚',\n",
    "             'F': 'ğŸ‡«',\n",
    "             'R': 'ğŸ‡·',\n",
    "             'U': 'ğŸ‡º',\n",
    "             'S': 'ğŸ‡¸',\n",
    "             'Â°C': 'â„ƒ',\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "04a457b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = re.compile(r'[^\\w@#,\\.:;\\\\\\|/\\)\\(\\?!\\-â€‘â€•â€”â€“\\'\"â€˜â€™Â«Â»â€â€œâ€<>{}\\[\\]Â°%â„–&$â‚½â‚¬Â£Â®Â©â„¢Ã—~*+=\\^\\|`Â´]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37818d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet(tweet):\n",
    "    tweet_norm = re.sub(r'(\\xa0|\\u200a|\\u2028|\\u3000|\\u200f)', ' ', tweet)\n",
    "    tweet_norm = re.sub(r'\\xad', '', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&amp;', '&', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&gt;', '>', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&lt;', '<', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&quot;', '\"', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&laquo;', 'Â«', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&raquo;', 'Â»', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&#8230;', '...', tweet_norm)\n",
    "    tweet_norm = re.sub(r'&#13;', ' ', tweet_norm)\n",
    "    tweet_norm = re.sub(r'â€¦', '...', tweet_norm)\n",
    "    tweet_norm = re.sub(r'Ğ', 'Ğ•', tweet_norm)\n",
    "    tweet_norm = re.sub(r'Ñ‘', 'Ğµ', tweet_norm)\n",
    "    tweet_norm = re.sub(r'@[A-z0-9]+', 'USERNAME', tweet_norm) # Twitter handle -> USERNAME\n",
    "    tweet_norm = re.sub(r'https://t.co/[\\w\\d]+|HTTPURL_+|http[\\./\"\\':]*', ' HTTPS ', tweet_norm) # URL -> HTTPS\n",
    "    tweet_norm = re.sub(r'(pic)?\\.twitter\\.com/\\w+', ' HTTPS ', tweet_norm) # URL -> HTTPS\n",
    "    tweet_norm = re.sub(r'[0-9]+', 'NUM', tweet_norm) # number -> NUM\n",
    "    for symb in tweet_norm: # some emojis -> NEUTRAL_EMOJI / POS_EMOJI / NEG_EMOJI / HEALTH_EMOJI\n",
    "        for key in emoji_dict:\n",
    "            if symb in emoji_dict[key]:\n",
    "                tweet_norm = tweet_norm.replace(symb, key)\n",
    "    tweet_norm = re.sub(punct, ' ', tweet_norm) # everything except alphanumeric symbols and punctuation marks -> space\n",
    "    return tweet_norm.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbe83d",
   "metadata": {},
   "source": [
    "Preprocessing for NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "580473ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72d19657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(offsets, start_end_data):\n",
    "    encoded_labels = ['O'] * len(offsets)\n",
    "    for i in range(len(start_end_data)):\n",
    "        s = start_end_data[i]['start']\n",
    "        e = start_end_data[i]['end']\n",
    "        for idx, mapping in enumerate(offsets):\n",
    "            if mapping[0] == s:\n",
    "                encoded_labels[idx] = 'B'\n",
    "            elif mapping[0] > s and mapping[1] <= e:\n",
    "                encoded_labels[idx] = 'I'\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bb27dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens, subword_labels):\n",
    "    new_tokens = []\n",
    "    labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if not token.startswith('##'):\n",
    "            new_tokens.append(token)\n",
    "            labels.append(subword_labels[i])\n",
    "        else:\n",
    "            new_tokens[-1] += token[2:]\n",
    "    return new_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "021497b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_and_label(sent, ent, tokenizer):\n",
    "    encoding = tokenizer.encode_plus(sent,\n",
    "                                     return_offsets_mapping=True,\n",
    "                                     return_token_type_ids=False,\n",
    "                                     return_attention_mask=False,\n",
    "                                     add_special_tokens=False,\n",
    "                                    )\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "    offsets = encoding['offset_mapping']\n",
    "    subword_labels = encode_labels(offsets, ent)\n",
    "    words, labels = tokenize_text(tokens, subword_labels)\n",
    "    assert len(words) == len(labels)\n",
    "    return words, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70d1b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_labeled_texts(df, path, tokenizer):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write('-DOCSTART\\n')\n",
    "        texts = df['text'].tolist()\n",
    "        ents = df['ent'].tolist()\n",
    "        for i in range(len(texts)):\n",
    "            words, labels = tok_and_label(texts[i], ents[i], tokenizer)\n",
    "            for j in range(len(words)):\n",
    "                f.write(f'{words[j]} {labels[j]}\\n')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92e655",
   "metadata": {},
   "source": [
    "New span identification for normalized texts (NER task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c063bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_limits(text, span, lang='rus'):\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for i in range(len(text)):\n",
    "        if lang == 'eng':\n",
    "            new_text = text.lower()\n",
    "            new_span = span.lower()\n",
    "        elif lang == 'rus':\n",
    "            new_text = text\n",
    "            new_span = span\n",
    "        if new_text[i] == new_span[0]:\n",
    "            if new_text[i:i+len(new_span)] == new_span:\n",
    "                start = i\n",
    "                end = i + len(new_span)\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eec290",
   "metadata": {},
   "source": [
    "#### Normalization for classification task\n",
    "\n",
    "Input format: .tsv (tweet_id<font color='blue'>\\t</font>tweet<font color='blue'>\\t</font>label<font color='blue'>\\n</font>)\n",
    "\n",
    "Output format: .tsv (tweet_id<font color='blue'>\\t</font>normalized tweet<font color='blue'>\\t</font>label<font color='blue'>\\n</font>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07d09116",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('orig_train.tsv', sep='\\t', index_col='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "383d5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].apply(normalize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bda5110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2754bd",
   "metadata": {},
   "source": [
    "#### Preprocessing for NER task without normalization\n",
    "\n",
    "Input format: .tsv (tweet_id<font color='blue'>\\t</font>text<font color='blue'>\\t</font>start<font color='blue'>\\t</font>end<font color='blue'>\\t</font>span<font color='blue'>\\n</font>)\n",
    "\n",
    "Output format: .tsv (BIO scheme)<br>\n",
    "-DOCSTART<br>\n",
    "word1 B<br>\n",
    "word2 I<br>\n",
    "...<br>\n",
    "wordN O<br>\n",
    "\n",
    "word1 O<br>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04e7a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train = pd.read_csv('orig_train.tsv', sep='\\t', index_col=['tweet_id', 'text'])\n",
    "orig_train['ent'] = orig_train.apply(lambda x: {'start': x['start'], 'end': x['end'], 'span': x['span']}, axis=1)\n",
    "orig_train.drop(['start', 'end', 'span'], axis=1, inplace=True)\n",
    "train = orig_train.groupby(['tweet_id', 'text']).agg(list).reset_index().set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da7c8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_labeled_texts(train, 'train.tsv', tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc04730",
   "metadata": {},
   "source": [
    "#### Preprocessing and normalization for NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ea584dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train = pd.read_csv('orig_train.tsv', sep='\\t', index_col='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9ef1342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train['text'] = orig_train['text'].apply(normalize_tweet)\n",
    "orig_train['span'] = orig_train['span'].apply(normalize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a1a1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train['start'] = orig_train.apply(lambda x: find_limits(x['text'], x['span'], lang='rus')[0], axis=1)\n",
    "orig_train['end'] = orig_train.apply(lambda x: find_limits(x['text'], x['span'], lang='rus')[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfc59b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train['ent'] = orig_train.apply(lambda x: {'start': x['start'], 'end': x['end'], 'span': x['span']}, axis=1)\n",
    "orig_train.drop(['start', 'end', 'span'], axis=1, inplace=True)\n",
    "train = orig_train.groupby(['tweet_id', 'text']).agg(list).reset_index().set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d11a3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_labeled_texts(train, 'train.tsv', tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
